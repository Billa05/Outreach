{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffad1fb4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from xgboost import XGBClassifier\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import json\n",
    "\n",
    "# Download NLTK data\n",
    "nltk.download('stopwords', quiet=True)\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "# Step 1: Load the JSONL file\n",
    "def load_jsonl(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    return data\n",
    "\n",
    "# Replace with actual path to your JSONL file\n",
    "file_path = '/content/b2b_lead_data_india.jsonl'\n",
    "print(\"Loading data from JSONL file...\")\n",
    "data = load_jsonl(file_path)\n",
    "print(f\"Loaded {len(data)} records from {file_path}.\")\n",
    "\n",
    "# Convert to DataFrame\n",
    "print(\"Converting data to DataFrame...\")\n",
    "df = pd.DataFrame(data)\n",
    "print(f\"DataFrame created with shape: {df.shape}\")\n",
    "\n",
    "# Step 2: Preprocess the data\n",
    "print(\"Preprocessing data...\")\n",
    "df['company_text'] = df['org_summary']\n",
    "\n",
    "# Extract additional features from contact_info with safety checks\n",
    "df['has_contact_title'] = df['contact_info'].apply(lambda x: 1 if isinstance(x, dict) and x.get('contact_title') is not None else 0)\n",
    "df['has_phone'] = df['contact_info'].apply(lambda x: 1 if isinstance(x, dict) and x.get('phone') is not None else 0)\n",
    "df['has_email'] = df['contact_info'].apply(lambda x: 1 if isinstance(x, dict) and x.get('email') is not None else 0)\n",
    "\n",
    "# Add keyword overlap feature\n",
    "def keyword_overlap(query, summary):\n",
    "    query_words = set(query.lower().split()) - stop_words\n",
    "    summary_words = set(summary.lower().split()) - stop_words\n",
    "    if not query_words:\n",
    "        return 0.0\n",
    "    overlap = len(query_words.intersection(summary_words)) / len(query_words)\n",
    "    return overlap\n",
    "\n",
    "df['keyword_overlap'] = df.apply(lambda row: keyword_overlap(row['original_user_query'], row['org_summary']), axis=1)\n",
    "\n",
    "# Map labels: \"Good Fit\" -> 1, others -> 0\n",
    "df['label'] = df['user_feedback'].apply(lambda x: 1 if x == \"Good Fit\" else 0)\n",
    "print(\"Preprocessing completed. Features extracted and labels mapped.\")\n",
    "\n",
    "# Step 3: Generate embeddings using Hugging Face Sentence Transformers\n",
    "print(\"Initializing SentenceTransformer model...\")\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Embed the original_user_query and company_text (org_summary only)\n",
    "print(\"Generating embeddings for queries...\")\n",
    "query_embeddings = embedder.encode(df['original_user_query'].tolist(), convert_to_tensor=False)\n",
    "print(\"Generating embeddings for company texts...\")\n",
    "company_embeddings = embedder.encode(df['company_text'].tolist(), convert_to_tensor=False)\n",
    "\n",
    "# Compute cosine similarity with zero norm handling\n",
    "print(\"Computing cosine similarities...\")\n",
    "def safe_cosine_sim(q, c):\n",
    "    norm_q = np.linalg.norm(q)\n",
    "    norm_c = np.linalg.norm(c)\n",
    "    if norm_q == 0 or norm_c == 0:\n",
    "        return 0.0  # Handle zero norm case\n",
    "    return np.dot(q, c) / (norm_q * norm_c)\n",
    "\n",
    "cosine_sim = np.array([safe_cosine_sim(q, c) for q, c in zip(query_embeddings, company_embeddings)])\n",
    "print(\"Cosine similarities computed.\")\n",
    "\n",
    "# Combine embeddings and additional features\n",
    "print(\"Combining features...\")\n",
    "X_embeddings = np.hstack((query_embeddings, company_embeddings))\n",
    "X_additional = df[['has_contact_title', 'has_phone', 'has_email']].values\n",
    "X_sim = cosine_sim.reshape(-1, 1)\n",
    "X_overlap = df['keyword_overlap'].values.reshape(-1, 1)\n",
    "\n",
    "X = np.hstack((X_embeddings, X_additional, X_sim, X_overlap))\n",
    "print(f\"Feature matrix X created with shape: {X.shape}\")\n",
    "\n",
    "y = df['label'].values\n",
    "print(f\"Labels y created with shape: {y.shape}\")\n",
    "\n",
    "# Step 4: Scale features\n",
    "print(\"Scaling features...\")\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "print(\"Features scaled.\")\n",
    "\n",
    "# Step 5: Split data\n",
    "print(\"Splitting data into train and test sets...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "print(f\"Train set: {X_train.shape[0]} samples, Test set: {X_test.shape[0]} samples\")\n",
    "\n",
    "# Step 6: Handle imbalance with SMOTE\n",
    "print(\"Applying SMOTE oversampling...\")\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)\n",
    "print(f\"Resampled train shapes: X={X_train_resampled.shape}, y={np.bincount(y_train_resampled)}\")\n",
    "\n",
    "# Step 7: Hyperparameter tuning with GridSearchCV\n",
    "print(\"Running GridSearchCV for hyperparameter tuning...\")\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [3, 4],\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'scale_pos_weight': [1, 2, 3]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    XGBClassifier(random_state=42, eval_metric='logloss'),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1\n",
    ")\n",
    "grid_search.fit(X_train_resampled, y_train_resampled)\n",
    "model = grid_search.best_estimator_\n",
    "print(f\"Best params: {grid_search.best_params_}\")\n",
    "print(f\"Best CV F1-macro: {grid_search.best_score_:.3f}\")\n",
    "\n",
    "# Step 8: Cross-validation on full dataset for robust evaluation\n",
    "print(\"Running cross-validation on full dataset...\")\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(model, X, y, cv=cv, scoring='accuracy')\n",
    "print(f\"Cross-Validation Accuracy: {cv_scores.mean():.3f} (+/- {cv_scores.std() * 2:.3f})\")\n",
    "\n",
    "# Step 9: Evaluate the tuned model on test set\n",
    "print(\"Evaluating tuned model on test set...\")\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred, zero_division=0))\n",
    "print(\"Predicted Probabilities (Fit Scores) for Test Set:\\n\", y_pred_proba)\n",
    "\n",
    "# Step 10: Save the model and scaler\n",
    "import joblib\n",
    "joblib.dump(model, 'xgboost_lead_scorer_optimized.pkl')\n",
    "joblib.dump(scaler, 'feature_scaler_optimized.pkl')\n",
    "print(\"Optimized model and scaler saved successfully!\")\n",
    "\n",
    "# Step 11: Function for prediction on new leads\n",
    "def predict_fit_score(new_query, new_company_data):\n",
    "    print(\"Predicting fit score for new lead...\")\n",
    "    # new_company_data: dict with 'org_summary', 'contact_info'\n",
    "    new_company_text = new_company_data['org_summary']\n",
    "\n",
    "    new_has_contact_title = 1 if isinstance(new_company_data['contact_info'], dict) and new_company_data['contact_info'].get('contact_title') is not None else 0\n",
    "    new_has_phone = 1 if isinstance(new_company_data['contact_info'], dict) and new_company_data['contact_info'].get('phone') is not None else 0\n",
    "    new_has_email = 1 if isinstance(new_company_data['contact_info'], dict) and new_company_data['contact_info'].get('email') is not None else 0\n",
    "\n",
    "    print(\"Encoding new query and company text...\")\n",
    "    new_query_emb = embedder.encode([new_query])[0]\n",
    "    new_company_emb = embedder.encode([new_company_text])[0]\n",
    "\n",
    "    print(\"Computing cosine similarity for new lead...\")\n",
    "    new_cosine_sim = safe_cosine_sim(new_query_emb, new_company_emb)\n",
    "\n",
    "    new_overlap = keyword_overlap(new_query, new_company_text)\n",
    "\n",
    "    print(\"Combining features for new lead...\")\n",
    "    new_X_emb = np.concatenate((new_query_emb, new_company_emb))  # 1D array\n",
    "    new_X_emb = np.expand_dims(new_X_emb, axis=0)  # Reshape to 2D: (1, n)\n",
    "    new_X_additional = np.array([[new_has_contact_title, new_has_phone, new_has_email]])  # Shape: (1, 3)\n",
    "    new_X_sim = np.array([[new_cosine_sim]])  # Shape: (1, 1)\n",
    "    new_X_overlap = np.array([[new_overlap]])  # Shape: (1, 1)\n",
    "\n",
    "    new_X = np.hstack((new_X_emb, new_X_additional, new_X_sim, new_X_overlap))  # Shape: (1, n+5)\n",
    "    new_X = scaler.transform(new_X)\n",
    "\n",
    "    fit_score = model.predict_proba(new_X)[0][1] * 100\n",
    "    print(f\"Fit score calculated: {fit_score:.2f}%\")\n",
    "    return fit_score\n",
    "\n",
    "# Example usage\n",
    "print(\"Running example prediction...\")\n",
    "new_query = \"Find textile manufacturing companies in Bangalore looking for sustainable dyeing solutions.\"\n",
    "new_company_data = {\n",
    "    \"org_summary\": \"EcoFabrics is a Bangalore-based textile manufacturer focusing on sustainable dyeing and organic cotton fabrics.\",\n",
    "    \"contact_info\": {\"email\": \"sales@ecofabrics.com\", \"phone\": \"+91-80-1234-5678\", \"contact_title\": \"Sustainability Manager\"}\n",
    "}\n",
    "score = predict_fit_score(new_query, new_company_data)\n",
    "print(f\"Fit Score for new lead: {score:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
